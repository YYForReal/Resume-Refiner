import json
import os
from typing import List

import networkx as nx
import nltk
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from annotated_text import annotated_text, parameters
from streamlit_extras import add_vertical_space as avs
from streamlit_extras.badges import badge

from scripts.similarity.get_score import *
from scripts.utils import get_filenames_from_dir
from scripts.utils.logger import init_logging_config

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ResumeRefiner",
    page_icon="Assets/img/favicon.ico",
    initial_sidebar_state="auto",
)

init_logging_config()
cwd = find_path("Resume-Matcher")
config_path = os.path.join(cwd, "scripts", "similarity")

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

parameters.SHOW_LABEL_SEPARATOR = False
parameters.BORDER_RADIUS = 3
parameters.PADDING = "0.5 0.25rem"


def create_star_graph(nodes_and_weights, title):
    # åˆ›å»ºä¸€ä¸ªç©ºå›¾
    G = nx.Graph()

    # æ·»åŠ ä¸­å¿ƒèŠ‚ç‚¹
    central_node = "resume"
    G.add_node(central_node)

    # å‘å›¾ä¸­æ·»åŠ èŠ‚ç‚¹å’Œè¾¹åŠæƒé‡
    for node, weight in nodes_and_weights:
        G.add_node(node)
        G.add_edge(central_node, node, weight=weight * 100)

    # è·å–èŠ‚ç‚¹çš„å¸ƒå±€ä½ç½®
    pos = nx.spring_layout(G)

    # åˆ›å»ºè¾¹çš„è½¨è¿¹
    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])

    edge_trace = go.Scatter(
        x=edge_x,
        y=edge_y,
        line=dict(width=0.5, color="#888"),
        hoverinfo="none",
        mode="lines",
    )

    # åˆ›å»ºèŠ‚ç‚¹çš„è½¨è¿¹
    node_x = []
    node_y = []
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)

    node_trace = go.Scatter(
        x=node_x,
        y=node_y,
        mode="markers",
        hoverinfo="text",
        marker=dict(
            showscale=True,
            colorscale="Rainbow",
            reversescale=True,
            color=[],
            size=10,
            colorbar=dict(
                thickness=15,
                title="èŠ‚ç‚¹è¿æ¥æ•°",
                xanchor="left",
                titleside="right",
            ),
            line_width=2,
        ),
    )

    # æŒ‰è¿æ¥æ•°ç»™èŠ‚ç‚¹ç€è‰²
    node_adjacencies = []
    node_text = []
    for node in G.nodes():
        adjacencies = list(G.adj[node])
        node_adjacencies.append(len(adjacencies))
        node_text.append(f"{node}<br># è¿æ¥æ•°: {len(adjacencies)}")

    node_trace.marker.color = node_adjacencies
    node_trace.text = node_text

    # åˆ›å»ºå›¾å½¢
    fig = go.Figure(
        data=[edge_trace, node_trace],
        layout=go.Layout(
            title=title,
            titlefont_size=16,
            showlegend=False,
            hovermode="closest",
            margin=dict(b=20, l=5, r=5, t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        ),
    )

    # æ˜¾ç¤ºå›¾å½¢
    st.plotly_chart(fig)


def create_annotated_text(
    input_string: str, word_list: List[str], annotation: str, color_code: str
):
    # å¯¹è¾“å…¥å­—ç¬¦ä¸²è¿›è¡Œåˆ†è¯
    tokens = nltk.word_tokenize(input_string)

    # å°†åˆ—è¡¨è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
    word_set = set(word_list)

    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥ä¿å­˜æ³¨é‡Šæ–‡æœ¬
    annotated_text = []

    for token in tokens:
        # æ£€æŸ¥ token æ˜¯å¦åœ¨é›†åˆä¸­
        if token in word_set:
            # å¦‚æœåœ¨ï¼Œæ·»åŠ åŒ…å« tokenã€æ³¨é‡Šå’Œé¢œè‰²ä»£ç çš„å…ƒç»„
            annotated_text.append((token, annotation, color_code))
        else:
            # å¦‚æœä¸åœ¨ï¼Œç›´æ¥æ·»åŠ  token
            annotated_text.append(token)

    return annotated_text


def read_json(filename):
    with open(filename) as f:
        data = json.load(f)
    return data


def tokenize_string(input_string):
    tokens = nltk.word_tokenize(input_string)
    return tokens


# æ˜¾ç¤ºä¸»æ ‡é¢˜å’Œå­æ ‡é¢˜
st.title("ç®€å†ä¼˜åŒ–åŠ©æ‰‹")
with st.sidebar:
    st.image("Assets/img/header_image.png")
    st.subheader(
        "Datawhale AI å¤ä»¤è¥ ç¬¬å››æœŸ æµªæ½®ä¿¡æ¯æºå¤§æ¨¡å‹åº”ç”¨å¼€å‘â€”â€”AIç®€å†åŠ©æ‰‹"
    )
    st.markdown(
        "è®¿é—®ç½‘ç«™ [www.resumematcher.fyi](https://www.resumematcher.fyi/)"
    )

    st.markdown(
        "åœ¨ [GitHub](https://github.com/srbhr/resume-matcher) ç»™ Resume Matcher ç‚¹ä¸ª â­"
    )

    badge(type="github", name="srbhr/Resume-Matcher")
    st.markdown("å…³æ³¨æˆ‘ä»¥è·å–æœ€æ–°åŠ¨æ€ã€‚")
    badge(type="twitter", name="_srbhr_")
    st.markdown(
        "å¦‚æœæ‚¨å–œæ¬¢è¿™ä¸ªé¡¹ç›®å¹¶å¸Œæœ›è¿›ä¸€æ­¥æ”¯æŒå¼€å‘ï¼Œè¯·è€ƒè™‘ ğŸ‘‡"
    )
    badge(type="buymeacoffee", name="srbhr")

st.divider()
avs.add_vertical_space(1)

resume_names = get_filenames_from_dir("Data/Processed/Resumes")


st.markdown(
    f"##### å…±æœ‰ {len(resume_names)} ä»½ç®€å†ã€‚è¯·é€‰æ‹©ä»¥ä¸‹èœå•ä¸­çš„ä¸€ä»½ï¼š"
)
output = st.selectbox(f"", resume_names)


avs.add_vertical_space(5)

# st.write("ä½ é€‰æ‹©äº† ", output, " æ‰“å°ç®€å†")
selected_file = read_json("Data/Processed/Resumes/" + output)

avs.add_vertical_space(2)
st.markdown("#### è§£æåçš„ç®€å†æ•°æ®")
st.caption(
    "è¿™æ®µæ–‡æœ¬æ˜¯ä»ä½ çš„ç®€å†ä¸­è§£æå‡ºæ¥çš„ã€‚è¿™æ˜¯è¢« ATS è§£æåçš„æ ·å­ã€‚"
)
st.caption("åˆ©ç”¨è¿™ä¸ªä¿¡æ¯äº†è§£å¦‚ä½•ä½¿ä½ çš„ç®€å†æ›´é€‚åˆ ATSã€‚")
avs.add_vertical_space(3)
# st.json(selected_file)
st.write(selected_file["clean_data"])

avs.add_vertical_space(3)
st.write("ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä»ç®€å†ä¸­æå–çš„å…³é”®è¯ã€‚")

annotated_text(
    create_annotated_text(
        selected_file["clean_data"],
        selected_file["extracted_keywords"],
        "KW",
        "#0B666A",
    )
)

avs.add_vertical_space(5)
st.write("ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä»ç®€å†ä¸­æå–çš„å®ä½“ã€‚")

# è°ƒç”¨å‡½æ•°å¹¶ä¼ å…¥æ•°æ®
create_star_graph(selected_file["keyterms"], "ç®€å†ä¸­çš„å®ä½“")

df2 = pd.DataFrame(selected_file["keyterms"], columns=["å…³é”®è¯", "å€¼"])

# åˆ›å»ºå­—å…¸
keyword_dict = {}
for keyword, value in selected_file["keyterms"]:
    keyword_dict[keyword] = value * 100

fig = go.Figure(
    data=[
        go.Table(
            header=dict(
                values=["å…³é”®è¯", "å€¼"], font=dict(size=12), fill_color="#070A52"
            ),
            cells=dict(
                values=[list(keyword_dict.keys()), list(keyword_dict.values())],
                line_color="darkslategray",
                fill_color="#6DA9E4",
            ),
        )
    ]
)
st.plotly_chart(fig)

st.divider()

fig = px.treemap(
    df2,
    path=["å…³é”®è¯"],
    values="å€¼",
    color_continuous_scale="Rainbow",
    title="ä»ç®€å†ä¸­æå–çš„å…³é”®è¯/ä¸»é¢˜",
)
st.write(fig)

avs.add_vertical_space(5)

job_descriptions = get_filenames_from_dir("Data/Processed/JobDescription")


st.markdown(
    f"##### å…±æœ‰ {len(job_descriptions)} ä»½èŒä½æè¿°ã€‚è¯·é€‰æ‹©ä»¥ä¸‹èœå•ä¸­çš„ä¸€ä»½ï¼š"
)
output = st.selectbox("", job_descriptions)


avs.add_vertical_space(5)

selected_jd = read_json("Data/Processed/JobDescription/" + output)

avs.add_vertical_space(2)
st.markdown("#### èŒä½æè¿°")
st.caption(
    "ç›®å‰æˆ‘æ­£åœ¨ä» PDF ä¸­è§£æå®ƒï¼Œä½†æœªæ¥ä¼šä» txt æˆ–ç›´æ¥ç²˜è´´ã€‚"
)
avs.add_vertical_space(3)
# st.json(selected_file)
st.write(selected_jd["clean_data"])

st.markdown("#### èŒä½æè¿°å’Œç®€å†ä¸­çš„å¸¸è§è¯æ±‡å·²è¢«é«˜äº®æ˜¾ç¤ºã€‚")

annotated_text(
    create_annotated_text(
        selected_file["clean_data"], selected_jd["extracted_keywords"], "JD", "#F24C3D"
    )
)

st.write("ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä»èŒä½æè¿°ä¸­æå–çš„å®ä½“ã€‚")

# è°ƒç”¨å‡½æ•°å¹¶ä¼ å…¥æ•°æ®
create_star_graph(selected_jd["keyterms"], "èŒä½æè¿°ä¸­çš„å®ä½“")

df2 = pd.DataFrame(selected_jd["keyterms"], columns=["å…³é”®è¯", "å€¼"])

# åˆ›å»ºå­—å…¸
keyword_dict = {}
for keyword, value in selected_jd["keyterms"]:
    keyword_dict[keyword] = value * 100

fig = go.Figure(
    data=[
        go.Table(
            header=dict(
                values=["å…³é”®è¯", "å€¼"], font=dict(size=12), fill_color="#070A52"
            ),
            cells=dict(
                values=[list(keyword_dict.keys()), list(keyword_dict.values())],
                line_color="darkslategray",
                fill_color="#6DA9E4",
            ),
        )
    ]
)
st.plotly_chart(fig)

st.divider()

fig = px.treemap(
    df2,
    path=["å…³é”®è¯"],
    values="å€¼",
    color_continuous_scale="Rainbow",
    title="ä»é€‰å®šçš„èŒä½æè¿°ä¸­æå–çš„å…³é”®è¯/ä¸»é¢˜",
)
st.write(fig)

avs.add_vertical_space(3)

resume_string = " ".join(selected_file["extracted_keywords"])
jd_string = " ".join(selected_jd["extracted_keywords"])
result = get_score(resume_string, jd_string)
similarity_score = round(result[0].score * 100, 2)
score_color = "green"
if similarity_score < 60:
    score_color = "red"
elif 60 <= similarity_score < 75:
    score_color = "orange"
st.markdown(
    f"ç®€å†ä¸èŒä½æè¿°çš„ç›¸ä¼¼åº¦è¯„åˆ†ä¸º "
    f'<span style="color:{score_color};font-size:24px; font-weight:Bold">{similarity_score}</span>',
    unsafe_allow_html=True,
)

# è¿”å›é¡¶éƒ¨
st.markdown("[:arrow_up: è¿”å›é¡¶éƒ¨](#ç®€å†ä¼˜åŒ–åŠ©æ‰‹)")